<p align="center"><img src="https://github.com/SMILELab-FL/FedLab/raw/master/docs/imgs/FedLab-logo.svg?raw=True" width=600></p>

# FedLab-benchmarks
This repo contains standard FL algorithm implementations and FL benchmarks using [FedLab](https://github.com/SMILELab-FL/FedLab). 

Currently, following algorithms or benchrmarks are availableï¼š

## Optimization Algorithms
- [x] FedAvg: [Communication-Efficient Learning of Deep Networks from Decentralized Data](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf)
- [x] FedAsync: [Asynchronous Federated Optimization](http://arxiv.org/abs/1903.03934)
- [x] FedProx: [Federated Optimization in Heterogeneous Networks](https://arxiv.org/abs/1812.06127)
- [x] FedDyn: [Federated Learning based on Dynamic Regularization](https://openreview.net/pdf?id=B7v4QMR6Z9w)
- [x] Personalized-FedAvg: [Improving Federated Learning Personalization via Model Agnostic Meta Learning](https://arxiv.org/pdf/1909.12488.pdf)
- [x] qFFL: [Fair Resource Allocation in Federated Learning](https://arxiv.org/abs/1905.10497)
- [x] FedMGDA+: [Federated Learning meets Multi-objective Optimization](https://arxiv.org/abs/2006.11489)

## Compression Algorithms
- [x] DGC: [Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training](https://arxiv.org/abs/1712.01887)
- [x] QSGD: [Communication-Efficient SGD via Gradient Quantization and Encoding](https://proceedings.neurips.cc/paper/2017/hash/6c340f25839e6acdc73414517203f5f0-Abstract.html)

## Datasets
- [x] LEAF: [A Benchmark for Federated Settings](http://arxiv.org/abs/1812.01097)
- [x] NIID-Bench: [Federated Learning on Non-IID Data Silos: An Experimental Study](https://arxiv.org/abs/2102.02079)


------
**More reproductions of FL algorithms and settings are coming. And we welcome you to contribute federated learning algorithm based on FedLab. If you encounter any problems, do not hesitate to submit an issue or send an email to repo maintainers.**

